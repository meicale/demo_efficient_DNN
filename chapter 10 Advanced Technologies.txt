CHAPTER	10	Advanced Technologies

As highlighted throughout the previous chapters, data movement dominates energy consumption. The energy is consumed both in the access to the memory as well as the transfer of the data. The associated physical factors also limit the bandwidth available to deliver data between memory and compute, and thus limits the throughput of the overall system. This is commonly referred to by computer architects as the “memory wall.”1

To address the challenges associated with data movement, there have been various eﬀorts to bring compute and memory closer together. Chapters 5 and 6 primarily focus on how to design spatial architectures that distribute the on-chip memory closer to the computation (e.g., scratch pad memory in the PE). This chapter will describe various other architectures that use advanced memory, process, and fabrication technologies to bring the compute and memory together. 

First, we will describe eﬀorts to bring the oﬀ-chip high-density memory (e.g., DRAM) closer to the computation. These approaches are often referred to as processing near memory or near-data processing, and include memory technologies such as embedded DRAM and 3-D stacked  DRAM.

Next, we will describe eﬀorts to integrate the computation into the memory itself. These approaches are often referred to as processing in memory or in-memory computing, and include memory technologies such as Static Random Access Memories (SRAM), Dynamic Random Access Memories (DRAM), and emerging non-volatile memory (NVM). Since these approaches rely on mixed-signal circuit design to enable processing in the analog domain, we will also discuss the design challenges related to handling the increased sensitivity to circuit and device non-idealities (e.g., nonlinearity, process and temperature variations), as well as the impact on area density, which is critical for memory.

Signiﬁcant data movement also occurs between the sensor that collects the data and the DNN processor. The same principles that are used to bring compute near the memory, where the weights are stored, can be used to bring the compute near the sensor, where the input data is collected. Therefore, we will also discuss how to integrate some of the compute into the sensor.

Finally, since photons travel much faster than electrons and the cost of moving a photon can be independent of distance, processing in the optical domain using light may provide signiﬁcant improvements in energy eﬃciency and throughput over the electrical domain. Accordingly, we will conclude this chapter by discussing the recent work that performs DNN processing in the optical domain, referred to as Optical Neural Networks.

------
1Speciﬁcally, the memory wall refers to data moving between the oﬀ-chip memory (e.g., DRAM) and the processor.	
------

Table 10.1: Example of recent works that explore processing near memory. For I/O, TSV refers to through-silicon vias, while TCI refers to ThruChip Interface which uses inductive coupling. For bandwidth, ch refers to number of parallel communication channels, which can be the number of tiles (for eDRAM) or the number of vaults (for stacked memory). The size of stacked DRAM is based on Hybrid Memory Cube (HMC) Gen2 speciﬁcations.

10.1    PROCESSING NEAR MEMORY 

High-density memories typically require a diﬀerent process technology than processors and as a result are often fabricated as separate chips; as a result, accessing high-density memories requires going oﬀ-chip. The bandwidth and energy cost of accessing high-density oﬀ-chip memories are often limited by the number of I/O pads per chip and the oﬀ-chip interconnect channel characteristics (i.e., its resistance, inductance, and capacitance). Processing near memory aims to overcome these limitations by bringing the compute near the high-density memory to reduce access energy and increase memory bandwidth. The reduction in access energy is achieved by reducing the length of the interconnect between the memory and compute, while the increase in bandwidth is primarily enabled by increasing the number of bits that can be accessed per cycle by allowing for a wider interconnect and, to a lesser extent, by increasing the clock frequency, which is made possible by the reduced interconnect length.

Various recent advanced memory technologies aim to enable processing near memory with diﬀering integration costs. Table 10.1 summarizes some of these eﬀorts, where high-density memories on the order of tens of megabytes to gigabytes are connected to the compute engine at bandwidths of tens to hundreds of gigabytes per second. Note that currently most academic evaluations of DNN systems using advanced memory technologies have been based on simulations rather than fabrication and measurements.

In this section, we will describe the cost and beneﬁts of each technology and provide examples of how they have been used to process DNNs. The architectural design challenges of using processing-near-memory include how to allocate data to memory since the access patterns for high-density memories are often limited (e.g., data needs to be divided into diﬀerent banks and vaults in the DRAM or stacked DRAM, respectively), how to design the network-onchip between the memory and PEs, how to allocate the chip area between on-chip memory and compute now that oﬀ-chip communication less expensive, and how to design the memory hierarchy and dataﬂow now that the data movement costs are diﬀerent.

10.1.1	EMBEDDED HIGH-DENSITY MEMORIES 
Accessing data from oﬀ-chip memory can result in high energy cost as well as limited mem ory bandwidth (due to limited data bus width due to number of I/O pads, and signaling frequency due to the channel characteristics of the oﬀ-chip routing). Therefore, there has been a signiﬁcant amount of eﬀort toward embedding high-density memory on-chip. This includes technology such as embedded DRAM (eDRAM) [320] as well as embedded non-volatile (eNVM) [321], which includes embedded Flash (eFlash) [322], magnetic random-access memory (MRAM) [323], resistive random-access memory (RRAM) [324, 325], and phase change memory (PCRAM) [326].

In DNN processing, these high-density memories can be used to store tens of megabytes of weights and activations on chip to reduce oﬀ-chip access. For instance, DaDianNao [152] uses 16×2MB eDRAM tiles to store the weights and 2×2MB eDRAM tiles to store the input and output activations; furthermore, all these tiles (each with 4096-bit rows) can be accessed in parallel, which gives extremely high memory bandwidth.2 The downside of eDRAM is that it has a lower density than oﬀ-chip DRAM and can increase the fabrication cost of the chip. In addition, it has been reported that eDRAM scaling is slower than SRAM scaling [327], and thus the density advantage of eDRAM over SRAM will reduce over time. In contrast, eNVMs have gained popularity in recent years due to its increased density as well as its nonvolatility properties and reduction in standby power (e.g., leakage, refresh, etc.) compared to eDRAM [327].

10.1.2	STACKED MEMORY (3-D MEMORY ) 
Rather than integrating DRAM into the chip itself, the DRAM can also be stacked on top of  the chip using through-silicon vias (TSVs). This technology is often referred to as 3-D memory,3 and has been commercialized in the form of Hybrid Memory Cube (HMC) [328] and High Bandwidth Memory (HBM) [122]. 3-D memory delivers an order of magnitude higher bandwidth and reduces access energy by up to 5× relative to existing 2-D DRAMs, as TSVs have lower capacitance than typical oﬀ-chip interconnects.

------
2DaDianNao [152] assumes that the DNN model can ﬁt into the 32MB of eDRAM allocated to the weights. In practice, this implies that the design either limits the size of DNN model, or requires access to oﬀ-chip memory if the size of the DNN model exceeds the capacity of the eDRAM.
3Also referred to as “in-package” memory since both the memory and compute can be integrated into the same package.
------

Recent works have explored the use of HMC for eﬃcient DNN processing in a variety of ways. For instance, Neurocube [316], shown in Figure 10.1a, uses HMC to bring the memory and computation closer together. Each DRAM vault (vertically stacked DRAM banks) is connected to a PE containing a buﬀer and several MACs. A 2-D mesh network-on-chip (NoC) is used to connect the diﬀerent PEs, allowing the PEs to access data from diﬀerent vaults. One major design decision involves determining how to distribute the weights and activations across the diﬀerent vaults to reduce the traﬃc on the NoC.
 
Figure 10.1: Stacked memory systems. (a) DRAM using through-silicon vias (TSV ) and
(b) SRAM using inductive coupling. 

Another example that uses HMC is Tetris [317], which explores the use of HMC with the Eyeriss spatial architecture and row-stationary dataﬂow. It proposes allocating more area to computation than on-chip memory (i.e., larger PE array and smaller global buﬀer) in order to exploit the low-energy and high-throughput properties of the HMC. It also adapts the dataﬂow to account for the HMC and smaller on-chip memory.

SRAM can also be stacked on top of the chip to provide 10× lower latency compared to DRAM [318]. For instance, Quest [318], shown in Figure 10.1b, uses eight 3-D stacked SRAM dies to store both the weights and the activations of the intermediate feature maps when processing layer by layer. The SRAM dies are connected to the chip using inductive-coupling die-to-die wireless communication technology, known as a ThruChip Interface (TCI) [330], which has lower integration cost than TSV.

The above 3-D memory designs involve using TSV or TCI to connect memory and logic dies that have been separately fabricated. Recent breakthroughs in nanotechnology have made it feasible to directly fabricate thin layers of logic and memory devices on top of each other, referred to as monolithic 3-D integration. Interlayer vias (ILVs), which have several orders of magnitude denser vertical connectivity than TSV, can then be used to connect the memory and compute. Current monolithic 3-D integration systems, such as N3XT, use on-chip non-volatile memory (e.g., resistive RAM (RRAM), spin-transfer torque RAM (STT-RAM)/magnetic RAM (MRAM), phase change RAM (PCRAM)), and carbon nanotube logic (CNFET). Based on simulations, the energy-delay product of ILVs can be up to two orders of magnitude lower than 2-D systems on deep neural network workloads, compared to 8× for TSV [319].4

Figure 10.2: Comparison of conventional processing and processing in memory. 

In order to fully understand the impact of near memory processing it is important to analyze the impact that the added storage layer has on the mappings that are now available. Speciﬁcally, the new memories are faster, but also smaller, so optimal mappings will be diﬀerent.

10.2	PROCESSING IN MEMORY

While the previous section discussed methods to bring the compute near the memory, this section discusses processing in memory, which brings the compute into the memory. We will ﬁrst highlight the diﬀerences between processing in memory and conventional architectures, then describe how processing in memory can be performed using diﬀerent memory technologies including NVM, SRAM, and DRAM. Finally, we will highlight various design challenges associated with processing-in-memory accelerators that are commonly found across technologies.

DNN processing can be performed using matrix-vector multiplication (see Figures 4.2 and 4.3), as discussed in Chapter 4. For conventional architectures, both the input activation vector and the weight matrix are read out from their respective memories and processed by a MAC array, as shown in Figure 10.2a; the number of weights that can be read at once is limited by the memory interface (e.g., the read out logic and the number of memory ports). This limited memory bandwidth for the weights (e.g., a row of A weights per cycle in Figure 10.2b) can also limit the number of MAC operations that can be performed in parallel (i.e., operations per cycle) and thus the overall throughput (i.e., operations per second).

------
4The savings are highest for DNN models and conﬁgurations with low amounts of data reuse (e.g., FC layers with small batch size) resulting in more data movement across ILV.
------

Processing-in-memory architectures propose moving the compute into the memory that stores the weight matrix, as shown in Figure 10.2b. This can help reduce the data movement of the weights by avoiding the cost of reading the weight matrix; rather than reading the weights, only the computed results such as the partial sums or the ﬁnal output activations are read out of the memory. Furthermore, processing in memory architectures can also increase the memory bandwidth, as the number of weights that can be accessed in parallel is no longer limited by the memory interface; in theory, the entire weight matrix (e.g., A × B in Figure 10.2b) could be read and processed in parallel.

Figure 10.3 shows a weight-stationary dataﬂow architecture that is typically used for processing in memory. The word lines (WLs) are used to deliver the input activations to the storage elements, and the bit lines (BLs) are used to read the computed output activations or partial sums. The MAC array is implemented using the storage elements (that store the weights), where a multiplication is performed at each storage element, and the accumulation across multiple storage elements on the same column is performed using the bit line. In theory, a MAC array of B rows of A elements can access all A × B weights at the same time, and perform up to A dot products in parallel, where each sums B elements (i.e., A × B MAC operations per cycle).

Similar to other weight-stationary architectures, the input activations can be reused across the diﬀerent columns (up to A times for the example given in Figure 10.3), which reduces number of input activation reads. In addition, since a storage element tends to be smaller in area than the logic for a digital MAC (10 to 100× smaller in area and 3 to 10× smaller in edge length [331]), the routing capacitance to deliver the input activations can also be reduced, which further reduces the energy cost of delivering the input activations. Depending on the format of the inputs and outputs to the array, digital-to-analog converters (DACs) and analog-to-digital converters (ADCs) may also be required to convert the word line and bit line values, respectively; the cost of the DAC scales with the precision of the input activations driven on the word line, while the cost of the ADC scales with the precision of the partial sums, which depends on the precision of the weights and input activations, and the number of values accumulated on the bit line (up to B).5

An alternative way to view processing in memory is to use the loop nest representation introduced in Chapter 5. Design 10.20 illustrates a processing-in-memory design for an FC layer with M output channels and where the input activations are ﬂattened along the input channel, height and width dimensions (CHW). The computation take place in one cycle computing all the results in a single cycle in line 7. For this design, some of the mapping constraints are that

------
5The number of bits that an ADC can correctly resolve also depends on its thermal noise (typically some multiple of kT/C, where k is the Boltzmann constant, T is the temperature, and C is the capacitance of the sampling capacitor). For instance, an N-bit ADC has 2N -1 decision boundaries (see Section 7.2.1). However, if the thermal noise is large, the location of the 2N -1 decision boundaries will move around, dynamically and randomly, and this will aﬀect the resulting accuracy of the DNN being processed. Therefore, designing a low noise ADC is an important consideration. Note that the thermal noise of the ADC scales with the power consumption and the area of the ADC. Accordingly, it is important that the ADC’s thermal noise be considered when evaluating the accuracy as demonstrated in [332–334], as the design of the ADC involves a trade-oﬀ between power, area, and accuracy.
-----

Figure 10.3: Typical dataﬂow for processing-in-memory accelerators. 

A >= M and B >= C × H × W .6 Note, that when A ¤ M or B ¤ C × H × W  under-utilization will occur, as described in Section 10.2.4.

------
6For this example, we disallow the cases where A < M or B < C × H × W , since that would require multiple passes and updates of the weights, which reduces the potential beneﬁts of processing in memory.
------

A processing in memory design can also handle convolutions as illustrated in the loop nest in Design 10.21. Here, we show a toy design of just a 1-D convolution with multiple input channels (C ) and multiple output channels (M ). The entire computation takes Q steps as the only temporal step is the for loop (line 8). Interpreting the activity in the body of the loop (line 10), we see that in each cycle all ﬁlter weights are used (M × S × C ) each as part a distinct MAC operation, the same input activation is used multiple times (C × S ) and multiple output partial sums are accumulated into (M ). This design reﬂects the Toeplitz expansion of the input activations (see Section 4.1), so the same input activations will be delivered multiple times, since the same value for the input activation index w will be generated for diﬀerent qs. For the processing in memory convolution design, some of the mapping constraints are that A 三 M and B 三 C × S . Note, that when A ¤ M or B ¤ C × S under-utilization will occur, as described in Section 10.2.4.

In the next few sections (Sections 10.2.1, 10.2.2, and 10.2.3), we will brieﬂy describe how processing in memory can be performed using diﬀerent memory technologies. Section 10.2.4 will then give an overview of some of the key design challenges and decisions that should be considered when designing processing-in-memory accelerators for DNNs. For instance, many of these designs are limited to reduced precision (i.e., low bit-width) due to the non-idealities of the devices and circuits used for memories.

10.2.1	NON-VOLATILE MEMORIES (NVM) 
Many recent works have explored enabling processing-in-memory using non-volatile memories  (NVM) due to their high density and thus potential for replacing oﬀ-chip memory and reducing oﬀ-chip data movement. Advanced non-volatile high-density memories use programmable resistive elements, commonly referred to as memristors [335], as storage elements. These NVM devices enable increased density since memory and computation can be densely packed with a similar density to DRAM [336].7

Non-volatile memories exploit Ohm’s law by using the conductance (i.e., the inverse of the resistance) of a device to represent a ﬁlter weight and the voltage across the device to represent the input activation value. So the resulting current can be interpreted as the product (i.e., a partial sum). This is referred to as a current-based approach. For instance, Figure 10.4a shows how a multiplication can be performed using the conductance of the NVM device as the weight, and the voltage on the word line as the input activation, and the current output to the bit line as the product of the two. The accumulation is done by summing the currents on the bit line based on Kirchhoﬀ ’s current law. Alternatively, for Flash-based NVM, the multiplication is performed using the current-voltage (IV ) characteristics of the ﬂoating-gate transistor, where the threshold voltage of the ﬂoating-gate transistor is set based on the weight, as shown in Figure 10.4c. Similar to the previously described approaches, a voltage proportional to the input activation can be applied across the device, and the accumulation is performed by summing output current of the devices on the bit line.

NVM-based processing-in-memory accelerators have several unique challenges, as described in [340, 341]. First, the cost of programming the memristors (i.e., writing to non-volatile memory) can be much higher than SRAM or DRAM; thus, typical solutions in this space require that the non-volatile memory to be suﬃciently large to hold all weights of the DNN model, rather than changing the weights in the memory for each layer or ﬁlter during processing.8 As discussed in Chapter 3, this may reduce ﬂexibility as it can limit the size of the DNN model that the accelerator can support.

Second, the NVM devices can also suﬀer from device-to-device and cycle-to-cycle variations with nonlinear conductance across the conductance range [340–342]. This aﬀects the number of bits that can be stored per device (typically 1 to 4) and the type of signaling used for the input and output activations. For instance, rather than encoding the input activation in terms of voltage amplitude, the input can also be encoded in time using pulse width modulation with a ﬁxed voltage (i.e., a unary coding), and the resulting current can be accumulated over time on a capacitor to generate the output voltage [343].

------
7To improve density, the resistive devices can be inserted between the cross-point of two wires and in certain cases can avoid the need for an access transistor [337]. Under this scenario, the device is commonly referred to as a cross-point element. 
8This design choice to hold all weights of the DNN is similar to the approach taken in some of the FPGA designs such as Brainwave [209] and FINN [226], where the weights are pinned on the on-chip memory of the FPGA during synthesis.
------

Figure 10.4: Performing a multiplication and accumulation using the storage element. Input activation is encoded as a voltage amplitude .Vi /. (a) For memristors, Gi is the conductance (i.e., 1/resistance) of a resistive device set according to the weight, and bit line current I is the accumulated partial sum value [329]. (b) The current-voltage (I-V ) characteristics of the resistive device. The slope of the curve is inversely proportional to the resistance (recall R D V =I ). Typically, the device can take on just two states: LRS is the low resistive state (also referred to as RON ) and HRS is the high resistive state (also referred to as ROFF ). (c) and (d) For ﬂoatinggate transistors, the multiplication is performed using its current-voltage (I-V ) characteristics, where the weight sets the threshold voltage (as illustrated by the diﬀerent color lines representing diﬀerent threshold voltages), and bit line current I is the accumulated partial sum value [339].
 
Finally, the NVM devices cannot have negative resistance, which presents a challenge for supporting negative weights. One approach is to represent signed weights using diﬀerential signaling that requires two storage elements per weight; accordingly, the weights are often stored using two separate arrays [344]. Another approach is to avoid using signed weights. For instance, in the case of binary weights, rather than representing the weights as [一1; 1] and performing binary multiplications, the weights can be represented as [0; 1] and perform XNOR logic operations, as discussed in Chapter 7, or NAND logic operations, as discussed in [345].

There are several popular candidates for NVM devices including phase change RAM (PCRAM), resistive RAM (RRAM or  ReRAM),  conductive  bridge  RAM  (CBRAM),  and spin transfer torque magnetic RAM (STT-MRAM) [346]. These devices have diﬀerent tradeoﬀs in terms of endurance (i.e., how many times it can be written), retention time (i.e., how often it needs to be refreshed and thus how frequently it needs to be written), write current (i.e., how much power is required to perform a write), area density (i.e., cell size), variations, and speed. An in-depth discussion of how these device properties aﬀect the performance of DNN processing can be found in [341]; Gokmen et al. [343] ﬂips the problem and describes how these devices should be designed such that they can be better suited for DNN processing.9

Recent works on NVM-based processing-in-memory accelerators have reported results from both simulation [329, 338, 347, 348] as well as fabricated test chips [344, 349]. While works based on simulation demonstrate functionality on large DNN models such as variants of VGGNet [73] for image classiﬁcation on ImageNet, works based on fabricated test chips still demonstrate functionality on simple DNN models for digit classiﬁcation on MNIST [344, 349]. Simulations often project capabilities beyond the current state-of-the-art. For instance, while works based on simulation often assume that all 128 or 256 rows can be activated at the same time, works based on fabricated test chips only activate up to 32 rows at once to account for process variations and limitations in the read out circuits (e.g., ADC); these limitations will be discussed more in Section 10.2.4. It should also be noted that fabricated test chips typically only use one bit per memristor [344, 349, 350].

10.2.2	STATIC RANDOM ACCESS MEMORIES (SRAM) 
Many recent works have explored the use of the SRAM bit cell to perform computation. They  can be loosely classiﬁed into current-based and charge-based designs.

Current-based designs use the current-voltage (IV ) characteristics of the bit cell to perform a multiplication, which is similar to the NVM current-based approach described in Section 10.2.1. For instance, Figure 10.5a shows how the input activation can be encoded as a voltage amplitude on the word line that controls the current through the pull-down network of a bit cell (IBC ) resulting in a voltage drop (VBL) proportional to the word line voltage [351]. The current from multiple bit cells (across diﬀerent rows on the same column) add together on the bit line to perform the accumulation [351]. The resulting voltage drop on the bit line is then proportional to the dot product of the weights and activations of the column.

------
9[341, 343] also describe how these devices might be used for training DNNs if the weights can be updated in parallel and in place within the memristor array.
------

The above current-based approach is susceptible to the variability and nonlinearity of the word line voltage-to-current relationship of the pull-down network in the bit cell; this create challenges in representing the weights precisely. Charge-based approaches avoid this by using charge sharing for the multiplication, where the computation based on the capacitance ratio between capacitors, which tends to be more linear and less sensitive to variations.

Figure 10.5b shows how a binary multiplication (i.e., XNOR) via charge sharing can be performed by conditionally charging up a local capacitor within a bit cell, based on the XNOR between the weight value stored in the bit cell and the input activation value that determines the word line voltage [352]. Accumulation can then be performed using charge sharing across the local capacitors of the bit cells on a bit line [352]. Other variants of this approach include performing the multiplication directly with the bit line [353], and charge sharing across diﬀerent bit lines to perform the accumulation [353–355].

One particular challenge that exists for SRAM-based processing-in-memory accelerators is maintaining bit cell stability. Speciﬁcally, the voltage swing on the bit line typically needs to be kept low in order to avoid a read disturb (i.e., accidentally ﬂipping the value stored in the bit cell when reading). This limits the voltage swing on the bit line, which aﬀects the number of bits that can be accumulated on the bit line for the partial sum; conventional SRAMs only resolve one bit on the bit line. One way to address this is by adding extra transistors to isolate the storage node in the bit cell from the large swing of the bit line [353]; however, this would increase the bit cell area and consequently reduce the overall area density.

Recent works on SRAM-based processing-in-memory accelerators have reported results from fabricated test chips [351–355]. In these works, they demonstrate functionality on simple DNN models for digit classiﬁcation on MNIST, often using layer-by-layer processing, where the weights are updated in the SRAM for each layer. Note that in these works, the layer shapes of the DNN model are often custom designed to ﬁt the array size of the SRAM to increase utilization; this may pose a challenge in terms of ﬂexibility, as discussed in Chapter 3.10

10.2.3	DYNAMIC RANDOM ACCESS MEMORIES (DRAM) 
Recent works have explored how processing in memory may be feasible using DRAM by per forming bit-wise logic operations when reading multiple bit cells. For instance, Figure 10.6 shows how AND and OR operations can be performed by accessing three rows in parallel [356]. When three rows are accessed at the same time, the output bit line voltage will depend on the average of the charge stored in the capacitors of the bit cells in three rows (note that the charge stored in capacitor of a bit cell depends on if the bit cell is storing a one or zero). Therefore, if the majority of the values of the bit cells are one (at least two out of three), then the output is a one; otherwise, the output is a zero. More precisely, if X , Y , and Z represent the logical values of the three cells, then the ﬁnal state of the bit line is X Y C Y Z C ZX . If Z D 1, then this is eﬀectively an OR operation between X and Y ; if Z D 0, then this is eﬀectively an AND operation between X and Y . The bit-wise logic operations can be built up into MAC operations across multiple cycles [357], similar to bit-serial processing described in Chapter 7. 

------
10It should be noted that since SRAM is less dense than typical oﬀ-chip memory (e.g., DRAM), they are not designed to replace oﬀ-chip memory or speciﬁcally addressing the “memory wall,” which pertains to oﬀ-chip memory bandwidth; instead, most SRAM-based processing-in-memory accelerators focus on reducing the on-chip data movement.
------

Figure 10.5: Performing a multiplication and accumulation using the storage element. (a) Multiplication can be performed using a SRAM bit-cell by encoding the input activation as a voltage amplitude on the word line that controls the current through the pull-down network of the bit cell (IBC ) resulting in a voltage drop (VBL) proportional to the word line voltage. If a zero (weight value of 一1) is stored in the bit cell, the voltage drop occurs on BL, while if a one (weight value of C1) is stored the voltage drop occurs on BLB. The current from multiple bit-cells within a column add together. (b) Binary multiplication (XNOR) is performed by connection transistors and local capacitor. Accumulation is performed by charge sharing across local capacitors in bit-cells from the same column.

Figure 10.6: Compute in DRAM based on charge sharing. Z controls whether an AND or OR is performed on input X and Y . At time t D 0, the local capacitor of the bit cells for X , Y , and Z are charged to VDD for one and 0 for zero, and the bit line is pre-charged to VDD=2. At time t D 1, the accessed transistors to the bit cells are enabled, and the capacitors are shorted together with the bit line. Charge sharing distributes the charge between the capacitors to ensure that the voltage across each capacitor is the same; therefore the resulting voltage on the bit line is proportional to the average charge across the three capacitors. If the majority of the capacitors stored at one (i.e., VDD), then the voltage on the bit line would be above VDD=2 (i.e., Cı); otherwise, the voltage on the bit line drops below VDD=2 (i.e., 一ı). At time t D 2, the sense ampliﬁers (SA) on the bit line amplify the voltage to full swing (i.e., VDD=2 C ı becomes VDD or VDD=2 一 ı becomes 0), such that the output of the logic function X Y C Y Z C ZX can be resolved on the bit line. Note that this form of computing is destructive, so we need to copy data beforehand.
 
It is important to note that the architecture of processing in memory with DRAM diﬀers from the processing in memory with NVM and SRAM (described in Sections 10.2.1 and 10.2.2, respectively) in that: (1) for DRAM, a bit-wise operation requires three storage elements from diﬀerent rows, whereas for NVM and SRAM, a MAC operation can be performed with a single storage element; and (2) for DRAM, only one bit-wise operation is performed per bit line and the accumulation occurs over time, whereas for NVM and SRAM, the accumulation of multiple MAC operations is performed on the bit line.11 As a result, for DRAM the parallel processing can only be enabled across bit lines (A in Figure 10.3), since only one operation can be performed per bit line, whereas for NVM and SRAM, the parallel processing can be enabled across both the bit lines and the word lines (A and B in Figure 10.3), since multiple operations can be performed per bit line. In addition, for DRAM, multiple cycles are required to build up a MAC operation from a bit-wise logic operation, which reduces throughput. Thus, a challenge for DRAM-based processing-in-memory accelerators is to ensure that there is suﬃcient parallelism across bit lines (A) to achieve the desired improvements in throughput.

Other challenges for DRAM-based processing-in-memory accelerators include variations in the capacitance in the diﬀerent bit cells, changing charge in capacitor of bit cell over time due to leakage, and detecting small changes in the bit line voltage. In addition, additional hardware may be required in the memory controller to access multiple rows at once and/or to convert the bit-wise logic operations to MAC operation, all of which can contribute to energy and area overhead.

While many of the recent works on DRAM-based processing-in-memory accelerators have been based on simulation [356, 357], it should be noted that performing AND and OR operations have been demonstrated on oﬀ-the shelf, unmodiﬁed, commercial DRAM [359]. This was achieved by violating the nominal timing speciﬁcation and activating multiple rows in rapid succession, which leaves multiple rows open simultaneously and enables charge sharing on the bit line.

10.2.4	DESIGN CHALLENGES 
Processing-in-memory accelerators oﬀer many potential beneﬁts including reduced data move ment of weights, higher memory bandwidth by reading multiple weights in parallel, higher throughput by performing multiple computations in parallel, and lower input activation delivery cost due to increased density of compute. However, there are several key design challenges and decisions that need to be considered in practice. Analog processing is typically required to bring the computation into the array of storage elements or into its peripheral circuits; therefore the major challenges for processing in memory are its sensitivity to circuit and device nonidealities (i.e., nonlinearity and process, voltage and temperature variations).12 Solutions to these challenges often require trade oﬀs between energy eﬃciency, throughput, area density, and accuracy,13 which reduce the achievable gains over conventional architectures. Architecture-level energy and area estimation tools such as Accelergy can be used to help evaluate some of these trade oﬀs [360].

------
11This bit-wise (bit-serial) approach has also been explored for SRAM [358].
------

In this section, when applicable we will use a toy example of a matrix vector multiplication based on a FC layer shown in Figure 10.7. A loop-nest representation of the design is shown in Design 10.22, where CHW D M D 4. In theory, the entire computation should only require one cycle as all the 16 weights can be accessed in parallel and all the 16 MAC operations can be performed in parallel.

Number of Storage Elements per Weight
Ideally, it would be desirable to be able to use one storage element (i.e., one device or bit cell) per weight to maximize density. In practice, multiple storage elements are required per weight due to the limited precision of each device or bit cell (typically on the order of 1 to 4 bits). As a result, multiple low-precision storage elements are used to represent a higher precision weight. Figure 10.8 shows how this applies to our toy example.

------
12Note that per chip training (i.e., diﬀerent DNN weights per chip instance) may help address nonlinearity and chip to chip variability, but is expensive in practice. In addition, while adapting the weights can help address static variability, dynamic variability, such as a change in temperature, remains a challenge.
13It should be noted that the loss in accuracy might not only be due to the reduced precision of the computations in the DNN model (discussed in Chapter 7), which can be replicated on a conventional processor, but also due to circuit/device nonidealities and limitations, including ADC precision and thermal noise. Unfortunately, these factors have rarely been decoupled during reporting in literature, which can make it diﬃcult to understand the design trade oﬀs.
------ 

Figure 10.7: Toy example of matrix vector multiplication for this section. This example uses an FC layer with N = 1, CHW = 4, and M = 4.

Figure 10.8: Example of multiple storage elements per weight. In our toy example we use 2 bits per weight so the storage cost goes from 4 × 4 to 4 × 8.

For non-volatile memories (e.g., RRAM), multiple storage elements can also be used per weight to reduce the eﬀect of devices variation (e.g., average 3 × 3 devices per weight [342]) or to represent a signed weight (i.e., since resistance is naturally non-negative, diﬀerential coding using two arrays is often used [342]). Finally, in the case of SRAMs, often additional transistors are required in the bit cell to perform an operation, which increases the area per bit cell. All of the above factors reduce the density and/or accuracy of the system.
 
Array Size
Ideally, it would be desirable to have a large array size (A × B) in order to allow for high weight read bandwidth and high throughput. In addition, a larger array size improves the area density by further amortizing the cost of the peripheral circuits, which can be signiﬁcant (e.g., the peripheral circuits, i.e., ADC and DAC, can account for over 50% of the energy consumption of NVM-based designs [329, 349]). In practice, the size of array limited by several factors.

1.	The resistance and capacitance of word line and bit line wires, which impacts robustness, speed, and energy consumption.
For instance, the bit line capacitance impacts robustness for charge domain approaches where charge sharing is used for accumulation, as a large bit line capacitance makes it diﬃcult to sense the charge stored on the local capacitor in the bit cell; the charge stored on the local capacitor can be an input value for DRAM-based designs or a product of weight and input activation for SRAM-based designs. An example of using charge sharing to sense the voltage across a local capacitor is shown in Figure 10.9. Speciﬁcally, the change in bit line voltage (!:1VBL) is

(10.1)
 
where Clocal and CBL are the capacitance of the local capacitor and bit line, respectively, and Vlocal is the voltage across the local capacitor (due to the charge stored on the local capacitor), and VDD is the supply voltage. If the local capacitor is only storing binary values, then Vlocal can either be VDD or 0. !:1VBL must be suﬃciently large such that we can measure any change in Vlocal; the more bits we want to measure on the bit line (i.e., bits of the partial sum or output activation), the larger the required !:1VBL. However, the size of Clocal is limited by the area density of the storage element; for instance, in [352], Clocal is limited to 1.2fF. As a result, the minimum value of !:1VBL limits the size of CBL, which limits the length of the bit line.

Similarly, the bit line resistance impacts robustness for current domain approaches where current summing is used for accumulation, as a large bit line resistance makes it diﬃcult to sense the change in the resistance in the NVM device, as shown in Figure 10.10. Speciﬁcally, the change in bit line voltage due to change on the resistance is
 
(10.2)
 
where RON and ROFF are the minimum and maximum resistance of the NVM device (proportional to the weight), respectively, RBL is the resistance of the bit line, and Vin is the input voltage (proportional to the input  activation).  The  ROFF  一 RON  is  limited  by  the NVM device [342]. As a result, the minimum value of !:1VBL limits the size of RBL, which again limits the length of the bit line.
 
Figure 10.9: Change in bit line voltage !:1VBL  is proportional to	Clocal . The bit line is 
precharged to VDD at t D 0, and we read the value on the local capacitor at t = 1.

Figure   10.10:   Change   in   bit   line   voltage   !:1VBL  D VHIGH 一 VLOW     is   proportional   to .RON CRBL /.ROFF CRBL / .  RON    (also  referred  to  as  LRS)  and  ROFF    (also  referred  to  as HRS) are the minimum and maximum resistance of the NVM device, respectively. 

2.	The utilization of the array will drop if the workload cannot ﬁll entire column or entire row, as shown in Figure 10.11a. If the DNN model has few weights per ﬁlter and does not require large dot products, e.g., C × H × W <= B, where C , H , and W , are the dimensions of the ﬁlter (FC layer), and B is the number of rows in the array, then there will be B - C × H × W idle rows in the array. If the DNN model has few output channels and does not have many dot products, e.g., M <= A, where M is the number of output channels and A is the number of columns in the array, then there will be A - M idle columns in the array.14 This becomes more of an issue when processing eﬃcient DNN models as described in Chapter 9, where the trend is to reduce the number of weights per ﬁlter. In digital designs, ﬂexible mapping can be used to increase utilization across diﬀerent ﬁlter shapes, as discussed in Chapter 6; however, this is much more challenging to implement in the analog domain. One option is to redesign the DNN model speciﬁcally for processing in memory with larger ﬁlters and fewer layers [315], which increases utilization of the array and reduces input activation data movement; however, the accuracy implications of such DNN models requires further study. Figure 10.11b shows how this applies to our toy example.

Figure 10.11: Array utilization. (a) Impact of array size on utilization. (b) Example of utilization if size of weight memory was 8 × 8. Even though in theory we should be able to perform 64 MAC operations in parallel, only 16 of the storage elements are used (utilization of 25%); as a result, only 16 MAC operations are performed in parallel, speciﬁcally, 4 dot products of 4 elements. 

------
14Note that if C × H × W > B or M > A, temporal tiling will need to be applied, as discussed in Chapter 4, and multiple passes (including updating weights in the array) will be required to complete the MAC operations. Furthermore, recall that if the completed sum (ﬁnal psum) can be computed within a single pass (i.e., C × H × W 三 B), then precision of the ADC can be reduced to the precision of the output activation. However, when multiple passes are needed, the ADC needs greater precision because the results of each pass need to be added together to form the completed sum; otherwise, there may be an accuracy loss.
------
 
As a result, typical fabricated array sizes range from 16b × 64 [353] to 512b × 512 [352] for SRAM and from 128 × 128 to 256 × 256 [342] for NVM. This limitation in array size aﬀects throughput, area density and energy eﬃciency. Multiple arrays can be used to scale up the design in order to ﬁt the entire DNN Model and increase throughput [329, 347]. However, the impact on amortizing the peripheral cost is minimal. Furthermore, an additional NoC must be introduced between the arrays. Accordingly, the limitations on energy eﬃciency and area density remain.

Number of Rows Activated in Parallel
Ideally, it would be desirable to use all rows (B) at once to maximize parallelism for high bandwidth and high throughput. In practice, the number of rows that can be used at once is limited to by several factors.
1.	The number of bits in the ADC, since more rows means more bits are required to resolve the accumulation (i.e., the partial sums will have more bits). Some works propose using fewer bits for ADC than the maximum required [361, 362], however, this can reduce the accuracy.15
2.	The cumulative eﬀect of the device variations can decrease the accuracy.
3.	The maximum voltage drop or accumulated current that can be tolerated by the bit line.16 This can be particularly challenging for advanced process technologies (e.g., 7 nm and below) due to the increase in bit line resistance and increased susceptibility to electromigration issues, which limits the maximum current on the bit line.

As a result, the typical number of rows activated in parallel is 64 [342] or below [344]. A digital accumulator can be used after each ADC to accumulate across all B rows in B/64 cycles [342]; however, this reduces throughput and increases energy due to multiple ADC conversions. To reduce the additional ADC conversion, recent work has explored performing the accumulation in the analog domain [348]. Figure 10.12 shows how this applies to our toy example. Design 10.23 shows the corresponding loop nest, and illustrates the multiple cycles it takes to perform all the MACs.

------
15The number of bits required by the ADC depends on the number of values being accumulated on the bit line (i.e., number of rows activated in parallel), whether the values are sparse [361] (i.e., zero values will not contribute to the accumulated sum), and whether the accumulated sum is a partial sum or a fully accumulated sum (i.e., it only needs to go through a nonlinear function to become an output activation). Using less than the maximum required ADC bits for the fully accumulated sum has less impact on accuracy than on the partial sum, since the fully accumulated sum is typically quantized to the bit-width of the input activation for the next layer, as discussed in Chapter 7. However, the ability to fully accumulate the sum on a bit line depends on the whether the number of rows in the array is large enough to hold all the weights for a given ﬁlter (i.e., B � C × H × W ).
16For instance, for a 6T SRAM bit cell, a large voltage drop on the bit line can cause the bit cell to ﬂip (i.e, an unwanted write operation on the bit cell); using 8T bit cell can prevent this at the cost of increased area.
------
 
Figure 10.12: Example of limited number of rows activated in parallel. If the ADC is only 3bits, only two rows can be used at a time. It would take two cycles (time steps) to complete the computation. There are two columns for psum in the ﬁgure: (1) psum (current cycle) corresponds to psum resulting from the dot product computed at the current cycle; (2) psum (accumulated) corresponds to the accumulated value of the psums across cycles. At t D 1, the psum of [6, 9, 3, 3] is computed and added (e.g., with a digital adder) to the psum at t D 0 of [1, 4, 7, 2] to achieve the ﬁnal psum [7, 13, 10, 5], as shown in the ﬁgure.

Design 10.23 Toy matrix multiply loop nest with limited number of parallel active rows

Number of Columns Activated in Parallel
Ideally, it would be desirable to use all columns (A) at once to maximize parallelism for high bandwidth and high throughput. In practice, the number of columns that can be used are limited by whether the area of ADC can pitch-match the width of the column, which is required for a compact area design; this can be challenging when using high-density storage elements such as NVM devices. A common solution is to time multiplex the ADC across a set of eight columns, which means that only A/8 columns are used in parallel [342]; however, this reduces throughput. Figure 10.13 shows how this applies to our toy example, and Design 10.24 shows the corresponding loop nest. 
 
Figure 10.13: Example of limited number of columns activated in parallel. If the width of an ADC is equal to two columns, then the columns need to be time multiplexed. It would take two cycles to complete the computation. If we combined this with the previously described parallel row limitations, it would take four cycles to complete the computation. 

Design 10.24 Toy matrix multiply loop nest with limited number of parallel active columns

Figure 10.14: Example of performing pulse-width modulation of the input activations with a 1-bit DAC. It would take three cycles to complete the computation if all weights can be used at once. Speciﬁcally, the input activations would be signaled across time as Œ1; 1; 0; 1] C Œ0; 1; 0; 1] C Œ0; 0; 0; 1] D Œ1; 2; 0; 3], where the width of the pulse in time corresponds to the value of the input. There are two columns for psum in the ﬁgure: (1) psum (current cycle) corresponds to psum resulting from the dot product computed at the current cycle; (2) psum (accumulated) corresponds to the accumulated value of the psums across cycles. Note that if we combined the limitation illustrated in this ﬁgure with the previously described parallel row and columns limitations, it would take 12 cycles to complete the computation.

Time to Deliver Input
Ideally, it would be desirable for all bits in the input activations to be encoded onto the word line in the minimum amount of time to maximize throughput; a typical approach is to use voltage amplitude modulation [351]. In practice, this can be challenging due to 
1.	the nonlinearity of devices makes encoding input value using voltage amplitude modulation diﬃcult, and 
2.	the complexity of the DAC that drives the word line scales with the number of bits
 
As a result, the input activations are often encoded in time (e.g., pulse-width modulation [354, 355] or number of pulses [362]17), with a ﬁxed voltage (DAC is only 1-bit) where the partial sum is determined by accumulating charge over time; however, this reduces throughput.18 Figure 10.14 shows how this applies to our toy example. One approach to reduce the complexity of the DAC or current accumulation time is to reduced the precision of the input activations, as discussed in Chapter 7; however, this will also reduce accuracy.

Time to Compute a MAC
Ideally, it would be desirable for a MAC to be computed in a single cycle. In practice, the storage element (bit cell or device) typically can only perform one-bit operations (e.g., XNOR and AND), and thus multiple cycles are required to build up to a multi-bit operation (e.g., full adder and multiplication) [331]. Figure 10.15 shows how this applies to our toy example. This also requires additional logic after the ADC to combine the one-bit operations into a multi-bit operation. However, this will reduce both the throughput, energy and density.

10.3	PROCESSING IN SENSOR

In certain applications, such as image processing, the data movement from the sensor itself can account for a signiﬁcant portion of the overall energy consumption. Accordingly, there has been work on bringing the processing near or into the sensor, which is similar to the work on bringing the processing near or into memory discussed in the previous sections. In both cases, the goal is to reduce the amount of data read out of the memory/sensor and thus the number of ADC conversions, which can be expensive. Both cases also require moving the computation into the analog domain and consequently suﬀer from increased sensitivity to circuit non-idealities. While processing near memory and processing in memory focus on reducing data movement of the weights of the DNN model, processing near sensor and processing in sensor focus on reducing the data movement of the inputs to the DNN model.

Processing near sensor has been demonstrated for image processing applications, where computation can be performed in the analog domain before the ADC in the peripheral of the image sensor. For instance, Zhang et al. [363] and Lee et al. [364] use switched capacitors to perform 4-bit multiplications and 3-bit by 6-bit MAC operations, respectively. RedEye [365] proposes performing the entire convolution layer (including convolution, max pooling and quantization) in the analog domain before the ADC. It should be noted that the results in [365] are based on simulations, while [363, 364] report measurements from fabricated test chips.

It is also feasible to embed the computation not just before the ADC, but directly into the sensor itself (i.e., processing in sensor). For instance, in [366] an Angle Sensitive Pixels sensor is used to compute the gradient of the input, which along with compression, reduces the data movement from the sensor by 10×. In addition, since the ﬁrst layer of the DNN often outputs a gradient-like feature map, it may be possible to skip the computations in the ﬁrst layer, which further reduces energy consumption, as discussed in [367, 368].

------
17Using pulses increases robustness to nonlinearity at the cost of increased switching activity. 18Alternatively, a single pulse can be used for the input activations if the weights are replicated across multiple rows (e.g., 2N -1 rows for an N-bit activation) [333]. This is a trade-oﬀ between time and area.
------

Figure 10.15: Example of time to compute MAC operation if each storage element can only perform one-bit operations. It takes three cycles to deliver the input (similar to Figure 10.14). There are two columns for psum in the ﬁgure: (1) psum (current cycle) corresponds to psum resulting from the dot product computed at the current cycle; (2) psum (accumulated) corresponds to the accumulated value of the psums across cycles. In addition, extra cycles are required at the end to combine accumulated bits from each bit line to form the ﬁnal output sum. The number of cycles required to perform the shift and add would depend on the number of bit lines divide by the number of sets of shift-and-add logic.

10.4	PROCESSING IN THE OPTICAL DOMAIN

Processing in the optical domain is an area of research that is currently being explored as an alternative to all-electronic accelerators [369]. It is motivated, in part, by the fact that photons travel much faster than electrons, and the cost of moving a photon can be independent of distance. Furthermore, multiplication can be performed passively (for example with optical interference [370, 371], with reconﬁgurable ﬁlters [373], or static phase masks [374]) and detection can occur at over 100 GHz. Thus, processing in the optical domain may provide signiﬁcant improvements in energy eﬃciency and throughput over the electrical domain.

Figure 10.16: Use ASP in front end to perform processing of ﬁrst layer. (Figure from [367].) 

Much of the recent work in the optical computing has focused on performing matrix multiplication, which can be used for DNN processing; these works are often referred to as photonic accelerators or optical neural networks. For instance, Shen et al. [370] present a programmable nanophotonic processor where the input activations are encoded in the amplitudes of optical pulses (light) that travel through an array of on-chip interferometers (composed of beamsplitters) that represent the weight matrix, where the weights determine the amount of light that is passed to the output. This is eﬀectively a weight-stationary dataﬂow. The accumulation is performed based on the accumulated light from various waveguides at the photodetector.

Alternatively, Hamerly et al. [371], shown in Figure 10.17b, demonstrate matrix multiplication based on coherent detection, where both the weights and activations are encoded on-the-ﬂy into light pulses, and are interfered in free-space on a beamsplitter to perform multiplication. Since, in this scheme, there is no need for on-chip interferometers (which have a large footprint), this approach may be more scalable, at the cost of added complexity in alignment. This is eﬀectively an output-stationary dataﬂow, where the output is accumulated on the photodetector as an analog electronic signal.

There is negligible power loss in the computation when processing in the optical domain. Most of the power dissipation occurs when converting between electrical and optical domains, speciﬁcally, in the converter to generate the light and the detector to collect the photons. There- fore, similar to the processing in memory work, the larger the array (or in this case the matrix), the more these conversion costs can be amortized.

Figure 10.17: Optical neural networks. 

Note, however, that while computing in the optical domain may be energy eﬃcient, the non-idealities in the optical devices (e.g., crosstalk between detectors, errors in phase encoding, photodetection noise) can lead to a reduction in accuracy. To address this accuracy loss, Bernstein et al. [372] propose a hybrid electronic-optics approach where the data transfer is done in the optical domain to exploit the distance-independent cost of photons, while the computation itself (i.e., MAC operation) is performed digitally in the electrical domain to avoid the non-idealities of the optical devices.

Recent works on optical neural networks have reported results based on simulations [371] or simulations based on data that has been extrapolated from experimental results [370]. These works demonstrate functionality on simple DNN models for digit classiﬁcation and vowel recognition. 