CHAPTER	11 Conclusion 

The use of deep neural networks (DNNs) has recently seen explosive growth. They are currently widely used for many artiﬁcial intelligence (AI) applications including computer vision, speech recognition, and robotics and are often delivering better than human accuracy. However, while DNNs can deliver this outstanding accuracy, it comes at the cost of high computational com- plexity. With the stagnation of improvements in general-purpose computation [11], there is a movement toward more domain-speciﬁc hardware, and in particular for DNN processing. Con- sequently, techniques that enable eﬃcient processing of DNNs to improve energy-eﬃciency and throughput without sacriﬁcing accuracy with cost-eﬀective hardware are critical to expanding the deployment of DNNs in both existing and new domains.

Creating a system for eﬃcient DNN processing should begin with understanding the current and future applications and the speciﬁc computations required for both now and the po- tential evolution of those computations. Therefore, this book surveyed a number of the current applications, focusing on computer vision applications, the associated algorithms, and the data being used to drive the algorithms. These applications, algorithms, and input data are experi- encing rapid change. So extrapolating these trends to determine the degree of ﬂexibility desired to handle next generation computations becomes an important ingredient of any design project. During the design-space exploration process, it is critical to understand and balance the important system metrics. For DNN computation these include the accuracy, energy, through- put and hardware cost. Evaluating these metrics is, of course, key, so this book surveyed the important components of a DNN workload. In speciﬁc, a DNN workload has two major com- ponents. First, the workload consists of the “network architecture” of the DNN model including the “shape” of each layer and the interconnections between layers. These can vary both within and between applications. Second, the workload consists of the speciﬁc data input to the DNN. This data will vary with the input set used for training or the data input during operation for inference.

This book also surveyed a number of avenues that prior work have taken to optimize DNN processing. Since data movement dominates energy consumption, a primary focus of some recent research has been to reduce data movement while maintaining accuracy, throughput, and cost. This means selecting architectures with favorable memory hierarchies like a spatial array, and developing dataﬂows that increase data reuse at the low-cost levels of the memory hierarchy. We have included a taxonomy of dataﬂows and an analysis of their characteristics. Understanding the throughput and energy eﬃciency of a DNN accelerator depends upon how each DNN workload maps to the hardware. Therefore, we discussed the process of optimally mapping workloads to the accelerator and the associated throughput and energy models.

The DNN domain also aﬀords an excellent opportunity for hardware/algorithm co- design. Many works have aimed to save storage space and energy by changing the representation of data values in the DNN. We distill and present the key concepts from these approaches. Still other work saves energy and sometimes increases throughput by increasing and then exploiting sparsity of weights and/or activations. We presented a new abstract data representation that en- ables a systematic presentation of designs focused on exploiting sparsity. Co-design needs to be aware of the impact on accuracy. Therefore, to avoid losing accuracy it is often useful to mod- ify the network or ﬁne-tune the network’s weights to accommodate these changes. Thus, this book both reviewed a variety of these techniques and discussed the frameworks that are available for describing, running and training networks.

Finally, DNNs aﬀord the opportunity to use mixed-signal circuit design and advanced technologies to improve eﬃciency. These include using memristors for analog computation and 3-D stacked memory. Advanced technologies can also facilitate moving computation closer to the source by embedding computation near or within the sensor and the memories. Of course, all of these techniques should also be considered in combination, while being careful to understand their interactions and looking for opportunities for joint hardware/algorithm co-optimization. 

In conclusion, although much work has been done, DNNs remain an important area of research with many promising applications and opportunities for innovation at various levels of hardware design. We hope this book provides a structured way of navigating the complex space of DNN accelerators designs that will inspire and lead to new advances in the ﬁeld.
